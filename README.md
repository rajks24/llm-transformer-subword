# Subword Transformer Language Model (LLM) - From Scratch with PyTorch

This project demonstrates how to build a **subword-level transformer-based language model** entirely from scratch using:

- ğŸ§± SentencePiece (BPE tokenization)
- ğŸ”¥ PyTorch TransformerEncoder
- ğŸŒ€ Top-k and Top-p sampling for generation
- ğŸ§ª WikiText-103 as source corpus

### ğŸ“ Project Structure

```shell
llm-transformer-subword/
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ tokenized_wikitext103.pt
â”‚Â Â  â””â”€â”€ wikitext103.txt
â”œâ”€â”€ notebook.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ wikitext_bpe.model
â””â”€â”€ wikitext_bpe.vocab
```

### âœ¨ Key Features

- âœ… Full tokenizer training using `SentencePieceTrainer`
- âœ… Custom transformer model with:
  - Positional encoding
  - Attention-based encoding blocks
  - Configurable size (layers, heads, embedding)
- âœ… Generation with:
  - Temperature
  - Top-k and Top-p filtering
- âœ… Output cleanup post-decoding for more natural text

### ğŸ“Œ Prompt Example

```text
Prompt: "In conclusion, the results of the analysis suggest"
Generated: "In conclusion, the results of the analysis suggest that the system was developed as a component of the overall design in a distributed manner..."
```

### ğŸ§° Tech Stack

- **Python 3.11+**
- **PyTorch** â€“ for model architecture, training loop, and tensor ops

| Library                                    | Purpose                                             |
| ------------------------------------------ | --------------------------------------------------- |
| `torch`, `torch.nn`, `torch.nn.functional` | Deep learning framework, model definition, training |
| `sentencepiece`                            | Subword tokenizer (BPE) for tokenizing text         |
| `datasets` (ğŸ¤— HuggingFace)                | To load datasets like WikiText-103                  |
| `tqdm`                                     | Progress bars during training                       |
| `os`, `math`, `time`                       | General utility modules (standard Python libs)      |
| `matplotlib.pyplot`                        | Plotting (e.g., attention weights)                  |
| `seaborn`                                  | Heatmaps for attention visualization                |

### ğŸ” Key Components Explained

### SentencePiece BPE Tokenizer

This project uses [SentencePiece](https://github.com/google/sentencepiece) to implement a subword tokenizer based on the Byte Pair Encoding (BPE) algorithm. Unlike word-level or character-level tokenizers, SentencePiece breaks down text into subword units, enabling:

- Robust handling of out-of-vocabulary words (e.g., "tokenization" â†’ `["â–token", "ization"]`)
- Reduced vocabulary size while preserving semantic structure
- Language-agnostic preprocessing (no need for whitespace tokenization)

In this project:

- We trained a BPE tokenizer on `wikitext103.txt` using `vocab_size=16,000` or `32,000`
- The trained model is saved as wikitext_bpe.model and used throughout for encoding and decoding
- Cleaned up decoding (e.g., replacing `@-@` with `-`, fixing punctuation) helps generate readable output

### PyTorch Transformer

The model is built from scratch using PyTorch's `TransformerEncoder` module:

- Custom positional encoding and attention blocks
- Configurable depth, width, and number of attention heads
- Final output is a language model trained to predict next subword tokens

### Hugging Face datasets

We use the ğŸ¤— `datasets` library to load WikiText-103, a large corpus of Wikipedia articles commonly used in LLM training. It provides:

- Fast, memory-efficient data access
- Easy dataset management via `load_dataset("wikitext", "wikitext-103-raw-v1")`

### ğŸ“š Ideal For

- NLP enthusiasts
- ML engineers exploring language models
- Anyone wanting to learn how LLMs work under the hood

ğŸ“Œ Credits
Created by _[Rajesh Singh](https://www.kaggle.com/rajinh)_ ğŸ˜€
